import { promises as fs } from 'fs'
import path from 'path'
import { createHash } from 'crypto'
import { getVectorCache, AIEmbedding } from './vector-cache'

export interface TextAnalysisResult {
  embedding: number[]
  modelName: string
  confidence: number
  processingTime: number
  wordCount: number
  charCount: number
  language?: string
  extractedText: string
  summary?: string
}

export interface TextMetadata {
  size: number
  hash: string
  encoding: string
  wordCount: number
  charCount: number
  language?: string
}

/**
 * AI í…ìŠ¤íŠ¸ ë¶„ì„ê¸°
 * OpenAI Embeddings API ë˜ëŠ” ë¡œì»¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ íŠ¹ì§•ì„ ì¶”ì¶œ
 */
export class AITextAnalyzer {
  private isInitialized = false
  private modelName = 'text-embedding-ada-002'
  private apiKey: string | null = null
  private useLocalModel = false

  /**
   * ë¶„ì„ê¸° ì´ˆê¸°í™”
   */
  async initialize(): Promise<void> {
    if (this.isInitialized) return

    try {
      console.log('ğŸ“ Initializing AI Text Analyzer...')

      // OpenAI API í‚¤ í™•ì¸
      this.apiKey = process.env.OPENAI_API_KEY || null

      if (this.apiKey) {
        console.log('ğŸ”‘ OpenAI API key found, using cloud embeddings')
        this.modelName = 'text-embedding-ada-002'
        this.useLocalModel = false
      } else {
        console.log('âš ï¸ No OpenAI API key found, using local embeddings')
        this.modelName = 'local-text-embeddings'
        this.useLocalModel = true
      }

      console.log('âœ… AI Text Analyzer initialized successfully')
      this.isInitialized = true
    } catch (error) {
      console.error('âŒ Failed to initialize AI Text Analyzer:', error)
      throw new Error(
        `Text analyzer initialization failed: ${error instanceof Error ? error.message : String(error)}`
      )
    }
  }

  /**
   * í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ë‚´ìš© ì¶”ì¶œ ë° ì „ì²˜ë¦¬
   */
  private async extractTextContent(filePath: string): Promise<{
    text: string
    metadata: TextMetadata
  }> {
    try {
      // íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
      try {
        await fs.access(filePath)
      } catch (accessError) {
        console.warn(`âš ï¸ File not found, skipping: ${filePath}`)
        return {
          text: '',
          metadata: {
            size: 0,
            hash: '',
            encoding: '',
            wordCount: 0,
            charCount: 0,
          },
        }
      }

      // íŒŒì¼ í†µê³„ ì •ë³´
      const stats = await fs.stat(filePath)

      // íŒŒì¼ ë‚´ìš© ì½ê¸° (ë‹¤ì–‘í•œ ì¸ì½”ë”© ì§€ì›)
      let content = ''
      const ext = path.extname(filePath).toLowerCase()

      if (ext === '.pdf') {
        const dataBuffer = await fs.readFile(filePath)
        // ì•ˆì •ì„±ì„ ìœ„í•´ ì„œë²„ ì‚¬ì´ë“œì—ì„œëŠ” pdfjs-distë§Œ ì‚¬ìš©
        content = await this.extractPdfTextWithPdfjs(dataBuffer)
      } else {
        try {
          // UTF-8ë¡œ ë¨¼ì € ì‹œë„
          content = await fs.readFile(filePath, 'utf-8')
        } catch (encodingError) {
          // UTF-8 ì‹¤íŒ¨ì‹œ ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
          try {
            content = await fs.readFile(filePath, 'latin1')
          } catch (fallbackError) {
            console.warn(
              `âš ï¸ Failed to read ${filePath} with standard encodings, using buffer`
            )
            const buffer = await fs.readFile(filePath)
            content = buffer.toString(
              'utf-8',
              0,
              Math.min(buffer.length, 10000)
            ) // ì²˜ìŒ 10KBë§Œ
          }
        }
      }

      // í…ìŠ¤íŠ¸ ì •ì œ
      const cleanedText = this.preprocessText(content)

      // í•´ì‹œ ê³„ì‚°
      const fileBuffer = await fs.readFile(filePath)
      const fileHash = createHash('md5').update(fileBuffer).digest('hex')

      // ë©”íƒ€ë°ì´í„° ìƒì„±
      const metadata: TextMetadata = {
        size: stats.size,
        hash: fileHash,
        encoding: 'utf-8', // ì¶”ì •
        wordCount: this.countWords(cleanedText),
        charCount: cleanedText.length,
        language: this.detectLanguage(cleanedText),
      }

      return { text: cleanedText, metadata }
    } catch (error) {
      throw new Error(
        `Failed to extract text from ${filePath}: ${error instanceof Error ? error.message : String(error)}`
      )
    }
  }

  /**
   * pdfjs-distë¥¼ ì´ìš©í•œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í´ë°±
   */
  private async extractPdfTextWithPdfjs(dataBuffer: Buffer): Promise<string> {
    try {
      // Node.js í™˜ê²½ì—ì„œëŠ” ë°˜ë“œì‹œ legacy ë¹Œë“œë¥¼ ì‚¬ìš©í•´ì•¼ ì•ˆì •ì 
      const pdfjs: any = await import('pdfjs-dist/legacy/build/pdf.mjs')

      // Node í™˜ê²½ì—ì„œ ì›Œì»¤ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ì„¤ì • (ì¼ë¶€ í™˜ê²½ì—ì„œ ì—ëŸ¬ ì˜ˆë°©)
      if (pdfjs.GlobalWorkerOptions) {
        try {
          pdfjs.GlobalWorkerOptions.workerSrc = undefined as any
        } catch {}
      }

      const loadingTask = pdfjs.getDocument({
        data: new Uint8Array(dataBuffer),
        isEvalSupported: false,
        useSystemFonts: true,
        disableFontFace: true,
      })
      const doc = await loadingTask.promise
      try {
        let fullText = ''
        for (let pageNum = 1; pageNum <= doc.numPages; pageNum++) {
          const page = await doc.getPage(pageNum)
          const textContent = await page.getTextContent()
          const pageText = (textContent.items as any[])
            .map((item: any) => (typeof item?.str === 'string' ? item.str : ''))
            .join(' ')
          fullText += pageText + '\n'
        }
        // pdfjsê°€ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í•˜ëŠ” ìŠ¤ìº”ë³¸ì˜ ê²½ìš° ë¹ˆ ë¬¸ìì—´ì´ ë  ìˆ˜ ìˆìŒ
        if (fullText.trim().length > 0) {
          return fullText
        }
        // ë¹ˆ ê²°ê³¼ë©´ ì™¸ë¶€ ë„êµ¬ í´ë°± ì‹œë„
        return await this.extractPdfTextWithPdftotext(dataBuffer)
      } finally {
        await doc.destroy()
      }
    } catch (fallbackError) {
      // pdfjs ì‹¤íŒ¨ ì‹œ ì™¸ë¶€ ë„êµ¬ í´ë°± ì‹œë„
      try {
        return await this.extractPdfTextWithPdftotext(dataBuffer)
      } catch (cliErr) {
        throw new Error(
          `pdfjs-dist extraction failed: ${fallbackError instanceof Error ? fallbackError.message : String(fallbackError)}`
        )
      }
    }
  }

  /**
   * popplerì˜ pdftotext CLIë¥¼ ì‚¬ìš©í•œ í´ë°± ì¶”ì¶œ (ì„¤ì¹˜ë˜ì–´ ìˆì„ ë•Œë§Œ ë™ì‘)
   */
  private async extractPdfTextWithPdftotext(
    dataBuffer: Buffer
  ): Promise<string> {
    try {
      const { execSync } = await import('child_process')
      // pdftotext ì¡´ì¬ í™•ì¸
      const whichOutput = execSync('which pdftotext', {
        encoding: 'utf8',
        stdio: 'pipe',
      }).trim()
      if (!whichOutput) {
        return ''
      }

      const os = await import('os')
      const tmpdir = os.tmpdir()
      const inputPath = `${tmpdir}/ai-text-${Date.now()}.pdf`
      const outputPath = `${tmpdir}/ai-text-${Date.now()}.txt`
      await fs.writeFile(inputPath, dataBuffer)

      try {
        execSync(
          `pdftotext -enc UTF-8 -layout -q "${inputPath}" "${outputPath}"`,
          {
            encoding: 'utf8',
            stdio: 'pipe',
            timeout: 60000,
          }
        )
      } catch {
        // ë³€í™˜ ì‹¤íŒ¨ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
        return ''
      } finally {
        try {
          await fs.rm(inputPath)
        } catch {}
      }

      try {
        const text = await fs.readFile(outputPath, 'utf-8')
        return text || ''
      } finally {
        try {
          await fs.rm(outputPath)
        } catch {}
      }
    } catch {
      return ''
    }
  }

  /**
   * í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
   */
  private preprocessText(text: string): string {
    return text
      .replace(/\r\n/g, '\n') // Windows ì¤„ë°”ê¿ˆ ì •ê·œí™”
      .replace(/\r/g, '\n') // Mac ì¤„ë°”ê¿ˆ ì •ê·œí™”
      .replace(/\n{3,}/g, '\n\n') // ê³¼ë„í•œ ì¤„ë°”ê¿ˆ ì œê±°
      .replace(/\s+/g, ' ') // ì—°ì†ëœ ê³µë°± ì •ê·œí™”
      .trim()
  }

  /**
   * ë‹¨ì–´ ìˆ˜ ê³„ì‚°
   */
  private countWords(text: string): number {
    return text.split(/\s+/).filter((word) => word.length > 0).length
  }

  /**
   * ê°„ë‹¨í•œ ì–¸ì–´ ê°ì§€
   */
  private detectLanguage(text: string): string {
    const sample = text.substring(0, 1000) // ì²˜ìŒ 1000ìë§Œ ìƒ˜í”Œë§

    // í•œê¸€ ë¬¸ì ë¹„ìœ¨
    const koreanChars = (sample.match(/[ê°€-í£]/g) || []).length
    const koreanRatio = koreanChars / sample.length

    // ì˜ì–´ ë¬¸ì ë¹„ìœ¨
    const englishChars = (sample.match(/[a-zA-Z]/g) || []).length
    const englishRatio = englishChars / sample.length

    if (koreanRatio > 0.1) return 'ko'
    if (englishRatio > 0.5) return 'en'
    return 'unknown'
  }

  /**
   * OpenAI Embeddings API í˜¸ì¶œ (ê°œì„ ëœ ë²„ì „)
   */
  private async getOpenAIEmbedding(text: string): Promise<number[]> {
    if (!this.apiKey) {
      throw new Error('OpenAI API key not available')
    }

    try {
      // í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (OpenAI í† í° ì œí•œ ê³ ë ¤)
      const maxTokens = 8000 // ëŒ€ëµ 8000 í† í°
      const truncatedText =
        text.length > maxTokens ? text.substring(0, maxTokens) : text

      console.log(
        `ğŸ”‘ Calling OpenAI API for text embedding (${truncatedText.length} chars)`
      )

      const response = await fetch('https://api.openai.com/v1/embeddings', {
        method: 'POST',
        headers: {
          Authorization: `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: this.modelName, // text-embedding-ada-002
          input: truncatedText,
          encoding_format: 'float', // ëª…ì‹œì ìœ¼ë¡œ float í˜•ì‹ ìš”ì²­
        }),
      })

      if (!response.ok) {
        const errorData = await response.json().catch(() => null)
        const errorMessage =
          errorData?.error?.message ||
          `${response.status} ${response.statusText}`
        throw new Error(`OpenAI API error: ${errorMessage}`)
      }

      const data = await response.json()

      if (!data.data || !data.data[0] || !data.data[0].embedding) {
        throw new Error('Invalid response format from OpenAI API')
      }

      const embedding = data.data[0].embedding
      console.log(
        `âœ… OpenAI embedding received: ${embedding.length} dimensions`
      )

      return embedding
    } catch (error) {
      console.error('âŒ OpenAI API call failed:', error)

      // API ì˜¤ë¥˜ì‹œ ë¡œì»¬ ì„ë² ë”©ìœ¼ë¡œ í´ë°±
      if (
        error instanceof Error &&
        error.message.includes('API') &&
        !error.message.includes('key')
      ) {
        console.warn('ğŸ”„ Falling back to local embedding due to API error')
        return await this.getLocalEmbedding(text)
      }

      throw error
    }
  }

  /**
   * OpenAI API ë°°ì¹˜ ì„ë² ë”© (ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬)
   */
  private async getOpenAIBatchEmbedding(texts: string[]): Promise<number[][]> {
    if (!this.apiKey) {
      throw new Error('OpenAI API key not available')
    }

    try {
      const maxTexts = 100 // OpenAI ë°°ì¹˜ ì œí•œ
      const batchTexts = texts
        .slice(0, maxTexts)
        .map((text) => (text.length > 8000 ? text.substring(0, 8000) : text))

      console.log(`ğŸ”‘ Calling OpenAI Batch API for ${batchTexts.length} texts`)

      const response = await fetch('https://api.openai.com/v1/embeddings', {
        method: 'POST',
        headers: {
          Authorization: `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: this.modelName,
          input: batchTexts,
          encoding_format: 'float',
        }),
      })

      if (!response.ok) {
        const errorData = await response.json().catch(() => null)
        const errorMessage =
          errorData?.error?.message ||
          `${response.status} ${response.statusText}`
        throw new Error(`OpenAI Batch API error: ${errorMessage}`)
      }

      const data = await response.json()

      if (!data.data || !Array.isArray(data.data)) {
        throw new Error('Invalid batch response format from OpenAI API')
      }

      const embeddings = data.data.map((item: any, index: number) => {
        if (!item.embedding) {
          throw new Error(`Missing embedding for text ${index}`)
        }
        return item.embedding
      })

      console.log(
        `âœ… OpenAI batch embeddings received: ${embeddings.length} texts, ${embeddings[0]?.length} dimensions each`
      )

      return embeddings
    } catch (error) {
      console.error('âŒ OpenAI Batch API call failed:', error)

      // ë°°ì¹˜ ì‹¤íŒ¨ì‹œ ê°œë³„ í˜¸ì¶œë¡œ í´ë°±
      console.warn('ğŸ”„ Falling back to individual API calls')
      const results = []
      for (const text of texts) {
        try {
          const embedding = await this.getOpenAIEmbedding(text)
          results.push(embedding)
        } catch (individualError) {
          console.warn(`âš ï¸ Skipping text due to error:`, individualError)
          const localEmbedding = await this.getLocalEmbedding(text)
          results.push(localEmbedding)
        }
      }
      return results
    }
  }

  /**
   * ë¡œì»¬ ì„ë² ë”© ìƒì„± (ê°„ë‹¨í•œ TF-IDF ê¸°ë°˜)
   */
  private async getLocalEmbedding(text: string): Promise<number[]> {
    console.log('ğŸ”„ Generating local text embedding...')

    // ê°„ë‹¨í•œ ë¬¸ì ê¸°ë°˜ íŠ¹ì§• ë²¡í„° ìƒì„±
    const words = text
      .toLowerCase()
      .split(/\s+/)
      .filter((word) => word.length > 2)
    const uniqueWords = [...new Set(words)]

    // ê³ ì • í¬ê¸° ë²¡í„° (1536ì°¨ì›ìœ¼ë¡œ OpenAIì™€ í˜¸í™˜)
    const vectorSize = 1536
    const embedding = new Array(vectorSize).fill(0)

    // ë‹¨ì–´ë“¤ì„ í•´ì‹œí•˜ì—¬ ë²¡í„° ì¸ë±ìŠ¤ì— ë§¤í•‘
    for (const word of uniqueWords.slice(0, 100)) {
      // ìƒìœ„ 100ê°œ ë‹¨ì–´ë§Œ ì‚¬ìš©
      const wordHash = this.simpleHash(word)
      const index = Math.abs(wordHash) % vectorSize
      embedding[index] += 1
    }

    // ì •ê·œí™”
    const magnitude = Math.sqrt(
      embedding.reduce((sum, val) => sum + val * val, 0)
    )
    if (magnitude > 0) {
      for (let i = 0; i < embedding.length; i++) {
        embedding[i] /= magnitude
      }
    }

    return embedding
  }

  /**
   * ê°„ë‹¨í•œ í•´ì‹œ í•¨ìˆ˜
   */
  private simpleHash(str: string): number {
    let hash = 0
    for (let i = 0; i < str.length; i++) {
      const char = str.charCodeAt(i)
      hash = (hash << 5) - hash + char
      hash = hash & hash // 32bit ì •ìˆ˜ë¡œ ë³€í™˜
    }
    return hash
  }

  /**
   * í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„± (ê°„ë‹¨í•œ ë²„ì „)
   */
  private generateSummary(text: string): string {
    const sentences = text.split(/[.!?]/).filter((s) => s.trim().length > 10)

    if (sentences.length <= 3) {
      return text.substring(0, 200) + (text.length > 200 ? '...' : '')
    }

    // ì²« ë²ˆì§¸ì™€ ì¤‘ê°„, ë§ˆì§€ë§‰ ë¬¸ì¥ ì„ íƒ
    const summary =
      [
        sentences[0],
        sentences[Math.floor(sentences.length / 2)],
        sentences[sentences.length - 1],
      ].join('. ') + '.'

    return summary.length > 300 ? summary.substring(0, 300) + '...' : summary
  }

  /**
   * í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ
   */
  async extractFeatures(filePath: string): Promise<TextAnalysisResult> {
    if (!this.isInitialized) {
      throw new Error('Text analyzer not initialized. Call initialize() first.')
    }

    const startTime = Date.now()

    try {
      console.log(`ğŸ“„ Analyzing text file: ${path.basename(filePath)}`)

      // í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
      const { text, metadata } = await this.extractTextContent(filePath)

      console.log(
        `ğŸ“Š Text stats: ${metadata.wordCount} words, ${metadata.charCount} chars, language: ${metadata.language}`
      )

      // ì„ë² ë”© ìƒì„±
      let embedding: number[]
      if (text.trim().length === 0) {
        // ë¹ˆ í…ìŠ¤íŠ¸ì˜ ê²½ìš° 1536ì°¨ì› ì˜ë²¡í„°ë¡œ ëŒ€ì²´í•˜ì—¬ íŒŒì´í”„ë¼ì¸ì„ ìœ ì§€
        embedding = new Array(1536).fill(0)
      } else if (this.useLocalModel) {
        embedding = await this.getLocalEmbedding(text)
      } else {
        embedding = await this.getOpenAIEmbedding(text)
      }

      // ìš”ì•½ ìƒì„±
      const summary = this.generateSummary(text)

      const processingTime = Date.now() - startTime

      console.log(
        `âœ… Text analysis completed: ${path.basename(filePath)} (${processingTime}ms)`
      )

      return {
        embedding,
        modelName: this.modelName,
        confidence: text.trim().length === 0 ? 0.1 : 0.9,
        processingTime,
        wordCount: metadata.wordCount,
        charCount: metadata.charCount,
        language: metadata.language,
        extractedText: text.substring(0, 1000), // ì²˜ìŒ 1000ìë§Œ ì €ì¥
        summary,
      }
    } catch (error) {
      console.error(`âŒ Failed to analyze text file ${filePath}:`, error)
      throw error
    }
  }

  /**
   * í…ìŠ¤íŠ¸ ë¶„ì„ ë° ìºì‹±
   */
  async analyzeAndCache(
    filePath: string,
    fileSize: number,
    fileHash: string
  ): Promise<AIEmbedding> {
    const vectorCache = await getVectorCache()

    // ìºì‹œì—ì„œ ê¸°ì¡´ ì„ë² ë”© í™•ì¸
    const existingEmbedding = await vectorCache.getEmbeddingByPath(filePath)
    if (
      existingEmbedding &&
      existingEmbedding.metadata &&
      existingEmbedding.metadata.hash === fileHash
    ) {
      console.log(
        `ğŸ“‹ Using cached text embedding for: ${path.basename(filePath)}`
      )
      return existingEmbedding
    }

    // ìƒˆë¡œìš´ ë¶„ì„ ìˆ˜í–‰
    const result = await this.extractFeatures(filePath)

    // ë©”íƒ€ë°ì´í„° êµ¬ì„±
    const metadata = {
      size: fileSize,
      hash: fileHash,
      width: 0, // í…ìŠ¤íŠ¸ íŒŒì¼ì—ëŠ” í•´ë‹¹ ì—†ìŒ
      height: 0,
      // í…ìŠ¤íŠ¸ íŠ¹í™” ë©”íƒ€ë°ì´í„°
      wordCount: result.wordCount,
      charCount: result.charCount,
      language: result.language,
      processingTime: result.processingTime,
      confidence: result.confidence,
      summary: result.summary,
    } as any

    // ìºì‹œì— ì €ì¥
    const embedding: AIEmbedding = {
      id: `text_${fileHash}_${Date.now()}`,
      filePath,
      fileType: 'text',
      embedding: result.embedding,
      modelName: result.modelName,
      extractedAt: new Date().toISOString(),
      metadata,
    }

    await vectorCache.saveEmbedding(embedding)
    return embedding
  }

  /**
   * ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ íŒŒì¼ ê²€ìƒ‰
   */
  async findSimilarTexts(
    queryPath: string,
    limit: number = 10,
    threshold: number = 0.7
  ): Promise<
    Array<{
      file: AIEmbedding
      similarity: number
    }>
  > {
    const vectorCache = await getVectorCache()

    // ì¿¼ë¦¬ íŒŒì¼ ë¶„ì„
    const fileStats = await fs.stat(queryPath)
    const fileBuffer = await fs.readFile(queryPath)
    const fileHash = createHash('md5').update(fileBuffer).digest('hex')

    const queryEmbedding = await this.analyzeAndCache(
      queryPath,
      fileStats.size,
      fileHash
    )

    // ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰
    const results = await vectorCache.findSimilar(
      queryEmbedding.embedding,
      'text',
      limit + 1,
      threshold
    )

    // ìê¸° ìì‹  ì œì™¸
    return results
      .filter((result) => result.file.filePath !== queryPath)
      .slice(0, limit)
      .map((result) => ({
        file: result.file,
        similarity: result.similarity,
      }))
  }

  /**
   * ë°°ì¹˜ í…ìŠ¤íŠ¸ ë¶„ì„
   */
  async analyzeBatch(
    textPaths: string[],
    progressCallback?: (
      completed: number,
      total: number,
      currentFile: string
    ) => void
  ): Promise<AIEmbedding[]> {
    const results: AIEmbedding[] = []
    const total = textPaths.length

    console.log(`ğŸ“š Starting batch text analysis of ${total} files...`)

    for (let i = 0; i < textPaths.length; i++) {
      const textPath = textPaths[i]

      try {
        if (progressCallback) {
          progressCallback(i, total, textPath)
        }

        // íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
        const fileStats = await fs.stat(textPath)
        const fileBuffer = await fs.readFile(textPath)
        const fileHash = createHash('md5').update(fileBuffer).digest('hex')

        // ë¶„ì„ ë° ìºì‹œ
        const embedding = await this.analyzeAndCache(
          textPath,
          fileStats.size,
          fileHash
        )
        results.push(embedding)

        console.log(
          `âœ… Processed ${i + 1}/${total}: ${path.basename(textPath)}`
        )
      } catch (error) {
        console.error(`âŒ Failed to process ${textPath}:`, error)
        // ì‹¤íŒ¨í•œ íŒŒì¼ì€ ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰
      }
    }

    if (progressCallback) {
      progressCallback(total, total, 'Completed')
    }

    console.log(
      `ğŸ Batch text analysis completed: ${results.length}/${total} files processed`
    )
    return results
  }

  /**
   * ëª¨ë¸ ì •ë³´ ë°˜í™˜
   */
  getModelInfo(): {
    name: string
    isInitialized: boolean
    useLocalModel: boolean
  } {
    return {
      name: this.modelName,
      isInitialized: this.isInitialized,
      useLocalModel: this.useLocalModel,
    }
  }

  /**
   * ë¦¬ì†ŒìŠ¤ ì •ë¦¬
   */
  dispose(): void {
    console.log('ğŸ§¹ Disposing text analyzer resources...')
    this.isInitialized = false
  }
}

// ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
let globalTextAnalyzer: AITextAnalyzer | null = null

export async function getTextAnalyzer(): Promise<AITextAnalyzer> {
  if (!globalTextAnalyzer) {
    globalTextAnalyzer = new AITextAnalyzer()
    await globalTextAnalyzer.initialize()
  }
  return globalTextAnalyzer
}
