import * as tf from '@tensorflow/tfjs-node'
import { promises as fs } from 'fs'
import sharp from 'sharp'
import { createHash } from 'crypto'
import { getVectorCache, AIEmbedding } from './vector-cache'

export interface ImageAnalysisResult {
  embedding: number[]
  modelName: string
  confidence: number
  processingTime: number
}

export class AIImageAnalyzer {
  private model: tf.LayersModel | null = null
  private modelName = 'mobilenet_v2'
  private isInitialized = false
  private isInitializing = false

  /**
   * ëª¨ë¸ ì´ˆê¸°í™”
   */
  async initialize(): Promise<void> {
    // ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆê±°ë‚˜ ì´ˆê¸°í™” ì¤‘ì¸ ê²½ìš°
    if (this.isInitialized) return
    if (this.isInitializing) {
      // ì´ˆê¸°í™”ê°€ ì§„í–‰ ì¤‘ì´ë©´ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
      while (this.isInitializing && !this.isInitialized) {
        await new Promise((resolve) => setTimeout(resolve, 100))
      }
      return
    }

    this.isInitializing = true

    try {
      console.log('ğŸ¤– Loading TensorFlow.js MobileNet model...')

      // ê¸°ì¡´ ëª¨ë¸ì´ ìˆë‹¤ë©´ ì •ë¦¬
      if (this.model) {
        console.log('ğŸ§¹ Disposing existing model...')
        this.model.dispose()
        this.model = null
      }

      // TensorFlow.js ë©”ëª¨ë¦¬ ì •ë¦¬ ë° ë°±ì—”ë“œ ì¤€ë¹„
      tf.disposeVariables()
      await tf.ready()

      console.log(`ğŸ”§ TensorFlow.js backend: ${tf.getBackend()}`)
      console.log(`ğŸ“Š Memory info: ${JSON.stringify(tf.memory())}`)

      // MobileNet v1 ëª¨ë¸ ë¡œë”© (ì•ˆì •ì ì¸ URL)
      console.log('ğŸ“¥ Downloading MobileNet model...')

      // ì—¬ëŸ¬ ì•ˆì •ì ì¸ ëª¨ë¸ URL ì‹œë„
      const modelUrls = [
        'https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v1_1.0_224/model.json',
        'https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet@1.0.0/dist/model.json',
      ]

      let modelLoaded = false
      for (const modelUrl of modelUrls) {
        try {
          console.log(`ğŸ“¥ Trying model URL: ${modelUrl}`)

          // ëª¨ë¸ ë¡œë”© (ë©”ëª¨ë¦¬ ê´€ë¦¬ ê°œì„ )
          this.model = await tf.loadLayersModel(modelUrl)

          modelLoaded = true
          console.log(`âœ… Model loaded successfully from: ${modelUrl}`)
          break
        } catch (error) {
          console.warn(
            `âš ï¸ Failed to load model from ${modelUrl}:`,
            (error as Error).message
          )
        }
      }

      if (!modelLoaded) {
        throw new Error(
          'Failed to load MobileNet model from all available URLs'
        )
      }

      console.log('âœ… MobileNet model loaded successfully')
      console.log(`ğŸ“Š Model input shape: ${this.model?.inputs[0].shape}`)
      console.log(`ğŸ“Š Updated memory info: ${JSON.stringify(tf.memory())}`)

      this.isInitialized = true
    } catch (error) {
      console.error('âŒ Failed to load AI model:', error)
      console.error('Error details:', {
        message: (error as Error).message,
        stack: (error as Error).stack,
        backend: tf.getBackend(),
        tfVersion: tf.version.tfjs,
        memoryInfo: tf.memory(),
      })

      this.isInitialized = false
      throw new Error(
        `AI model initialization failed: ${(error as Error).message}`
      )
    } finally {
      this.isInitializing = false
    }
  }

  /**
   * ì´ë¯¸ì§€ì—ì„œ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ
   */
  async extractFeatures(imagePath: string): Promise<ImageAnalysisResult> {
    if (!this.model) {
      throw new Error('Model not initialized. Call initialize() first.')
    }

    const startTime = Date.now()

    try {
      // ì´ë¯¸ì§€ ì „ì²˜ë¦¬
      const processedImage = await this.preprocessImage(imagePath)

      // íŠ¹ì§• ì¶”ì¶œ (ë§ˆì§€ë§‰ ë ˆì´ì–´ ì „ ë‹¨ê³„ì—ì„œ)
      const features = await this.extractImageFeatures(processedImage)

      // ë©”ëª¨ë¦¬ ì •ë¦¬
      processedImage.dispose()

      const processingTime = Date.now() - startTime

      console.log(
        `ğŸ¨ Image features extracted: ${imagePath} (${processingTime}ms)`
      )

      return {
        embedding: features,
        modelName: this.modelName,
        confidence: 1.0, // MobileNetì€ confidence ì ìˆ˜ë¥¼ ì§ì ‘ ì œê³µí•˜ì§€ ì•ŠìŒ
        processingTime,
      }
    } catch (error) {
      console.error(`âŒ Failed to extract features from ${imagePath}:`, error)
      throw error
    }
  }

  /**
   * ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (MobileNet ì…ë ¥ í˜•ì‹ì— ë§ê²Œ)
   */
  private async preprocessImage(imagePath: string): Promise<tf.Tensor4D> {
    try {
      // Sharpë¥¼ ì‚¬ìš©í•´ì„œ ì´ë¯¸ì§€ ì½ê¸° ë° ë¦¬ì‚¬ì´ì¦ˆ
      const imageBuffer = await sharp(imagePath)
        .resize(224, 224) // MobileNet ì…ë ¥ í¬ê¸°
        .removeAlpha() // ì•ŒíŒŒ ì±„ë„ ì œê±°
        .raw()
        .toBuffer()

      // Bufferë¥¼ Tensorë¡œ ë³€í™˜
      const tensor = tf.tensor3d(
        new Uint8Array(imageBuffer),
        [224, 224, 3],
        'int32'
      )

      // ì •ê·œí™” (0-255 -> 0-1) ë° ë°°ì¹˜ ì°¨ì› ì¶”ê°€
      const normalized = tensor.cast('float32').div(255.0)
      const batched = normalized.expandDims(0) as tf.Tensor4D

      // ë©”ëª¨ë¦¬ ì •ë¦¬
      tensor.dispose()
      normalized.dispose()

      return batched
    } catch (error) {
      console.error(`Failed to preprocess image ${imagePath}:`, error)
      throw error
    }
  }

  /**
   * MobileNetì—ì„œ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ
   */
  private async extractImageFeatures(
    imageTensor: tf.Tensor4D
  ): Promise<number[]> {
    if (!this.model) throw new Error('Model not initialized')

    try {
      // MobileNet ì˜ˆì¸¡ ì‹¤í–‰
      const prediction = this.model.predict(imageTensor) as tf.Tensor

      // í…ì„œ shape ë¡œê¹… (ë””ë²„ê¹…ìš©)
      console.log(
        `ğŸ“Š Prediction tensor shape: [${prediction.shape.join(', ')}]`
      )
      console.log(`ğŸ“Š Prediction tensor rank: ${prediction.rank}`)

      let features: tf.Tensor

      // í…ì„œ ì°¨ì›ì— ë”°ë¼ ì ì ˆí•œ ì²˜ë¦¬ ë°©ë²• ì„ íƒ
      if (prediction.rank === 4) {
        // 4ì°¨ì› í…ì„œ [batch, height, width, channels]ì˜ ê²½ìš°
        console.log('ğŸ”§ Processing 4D tensor with Global Average Pooling...')
        features = tf.mean(prediction, [1, 2]) // [batch, height, width, channels] -> [batch, channels]
      } else if (prediction.rank === 2) {
        // 2ì°¨ì› í…ì„œ [batch, features]ì˜ ê²½ìš° (ì´ë¯¸ flattenëœ ìƒíƒœ)
        console.log('ğŸ”§ Processing 2D tensor (already flattened)...')
        features = prediction
      } else if (prediction.rank === 3) {
        // 3ì°¨ì› í…ì„œ [batch, 1, channels] ë˜ëŠ” [batch, height, channels]ì˜ ê²½ìš°
        console.log('ğŸ”§ Processing 3D tensor...')
        // ë§ˆì§€ë§‰ ì°¨ì›ë§Œ ë‚¨ê¸°ê³  í‰ê· í™”
        features = tf.mean(prediction, [1])
      } else {
        throw new Error(
          `Unsupported tensor rank: ${prediction.rank}. Expected 2, 3, or 4 dimensions.`
        )
      }

      // ë°°ì¹˜ ì°¨ì› ì œê±° (ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬)
      const squeezed = features.squeeze([0])

      // í…ì„œë¥¼ JavaScript ë°°ì—´ë¡œ ë³€í™˜
      const featuresArray = await squeezed.data()
      const featuresVector = Array.from(featuresArray)

      console.log(`âœ… Features extracted: ${featuresVector.length} dimensions`)

      // ë©”ëª¨ë¦¬ ì •ë¦¬
      prediction.dispose()
      if (features !== prediction) {
        features.dispose()
      }
      squeezed.dispose()

      return featuresVector
    } catch (error) {
      console.error('Failed to extract features from tensor:', error)
      throw error
    }
  }

  /**
   * ì´ë¯¸ì§€ íŒŒì¼ ë¶„ì„ í›„ ë²¡í„° ìºì‹œì— ì €ì¥
   */
  async analyzeAndCache(
    imagePath: string,
    fileSize: number,
    fileHash: string
  ): Promise<AIEmbedding> {
    try {
      // ê¸°ì¡´ ìºì‹œ í™•ì¸
      const vectorCache = await getVectorCache()
      const embeddingId = this.generateEmbeddingId(imagePath, this.modelName)

      const existingEmbedding = await vectorCache.getEmbeddingByPath(
        imagePath,
        this.modelName
      )
      if (existingEmbedding) {
        console.log(`ğŸ“‹ Using cached embedding for: ${imagePath}`)
        return existingEmbedding
      }

      // ìƒˆë¡œ ë¶„ì„
      console.log(`ğŸ” Analyzing image: ${imagePath}`)
      const result = await this.extractFeatures(imagePath)

      // ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
      const metadata = await this.getImageMetadata(
        imagePath,
        fileSize,
        fileHash
      )

      // AIEmbedding ê°ì²´ ìƒì„±
      const embedding: AIEmbedding = {
        id: embeddingId,
        filePath: imagePath,
        fileType: 'image',
        modelName: result.modelName,
        embedding: result.embedding,
        extractedAt: new Date().toISOString(),
        metadata,
      }

      // ë²¡í„° ìºì‹œì— ì €ì¥
      await vectorCache.saveEmbedding(embedding)

      console.log(`âœ… Image embedding cached: ${imagePath}`)
      return embedding
    } catch (error) {
      console.error(`âŒ Failed to analyze and cache image ${imagePath}:`, error)
      throw error
    }
  }

  /**
   * ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
   */
  private async getImageMetadata(
    imagePath: string,
    fileSize: number,
    fileHash: string
  ) {
    try {
      const metadata = await sharp(imagePath).metadata()
      return {
        width: metadata.width,
        height: metadata.height,
        size: fileSize,
        hash: fileHash,
      }
    } catch (error) {
      console.warn(`Failed to get image metadata for ${imagePath}:`, error)
      return {
        size: fileSize,
        hash: fileHash,
      }
    }
  }

  /**
   * ì„ë² ë”© ID ìƒì„± (íŒŒì¼ ê²½ë¡œ + ëª¨ë¸ëª… ê¸°ë°˜)
   */
  private generateEmbeddingId(filePath: string, modelName: string): string {
    const content = `${filePath}:${modelName}`
    return createHash('md5').update(content).digest('hex')
  }

  /**
   * ìœ ì‚¬í•œ ì´ë¯¸ì§€ ê²€ìƒ‰
   */
  async findSimilarImages(
    queryImagePath: string,
    limit: number = 10,
    threshold: number = 0.7
  ): Promise<{ file: AIEmbedding; similarity: number }[]> {
    try {
      // ì¿¼ë¦¬ ì´ë¯¸ì§€ ë¶„ì„
      const queryResult = await this.extractFeatures(queryImagePath)

      // ë²¡í„° ìºì‹œì—ì„œ ìœ ì‚¬í•œ ì´ë¯¸ì§€ ê²€ìƒ‰
      const vectorCache = await getVectorCache()
      const results = await vectorCache.findSimilar(
        queryResult.embedding,
        'image',
        limit,
        threshold
      )

      return results.map((result) => ({
        file: result.file,
        similarity: result.similarity,
      }))
    } catch (error) {
      console.error(
        `Failed to find similar images for ${queryImagePath}:`,
        error
      )
      throw error
    }
  }

  /**
   * ë°°ì¹˜ ì´ë¯¸ì§€ ë¶„ì„
   */
  async analyzeBatch(
    imagePaths: string[],
    progressCallback?: (
      completed: number,
      total: number,
      currentFile: string
    ) => void
  ): Promise<AIEmbedding[]> {
    const results: AIEmbedding[] = []
    const total = imagePaths.length

    console.log(`ğŸš€ Starting batch analysis of ${total} images...`)

    for (let i = 0; i < imagePaths.length; i++) {
      const imagePath = imagePaths[i]

      try {
        if (progressCallback) {
          progressCallback(i, total, imagePath)
        }

        // íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
        const stats = await fs.stat(imagePath)
        const fileBuffer = await fs.readFile(imagePath)
        const fileHash = createHash('md5').update(fileBuffer).digest('hex')

        // ë¶„ì„ ë° ìºì‹œ
        const embedding = await this.analyzeAndCache(
          imagePath,
          stats.size,
          fileHash
        )
        results.push(embedding)

        console.log(`âœ… Processed ${i + 1}/${total}: ${imagePath}`)
      } catch (error) {
        console.error(`âŒ Failed to process ${imagePath}:`, error)
        // ì‹¤íŒ¨í•œ íŒŒì¼ì€ ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰
      }
    }

    if (progressCallback) {
      progressCallback(total, total, 'Completed')
    }

    console.log(
      `ğŸ Batch analysis completed: ${results.length}/${total} images processed`
    )
    return results
  }

  /**
   * ëª¨ë¸ ì •ë³´ ì¡°íšŒ
   */
  getModelInfo(): { name: string; isInitialized: boolean } {
    return {
      name: this.modelName,
      isInitialized: this.isInitialized,
    }
  }

  /**
   * ë©”ëª¨ë¦¬ ì •ë¦¬
   */
  dispose(): void {
    if (this.model) {
      this.model.dispose()
      this.model = null
    }
    this.isInitialized = false
    this.isInitializing = false
  }
}

// ì „ì—­ AI ì´ë¯¸ì§€ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
let globalImageAnalyzer: AIImageAnalyzer | null = null
let initializationPromise: Promise<AIImageAnalyzer> | null = null

export async function getImageAnalyzer(): Promise<AIImageAnalyzer> {
  // ì´ë¯¸ ì¸ìŠ¤í„´ìŠ¤ê°€ ìˆê³  ì´ˆê¸°í™”ë˜ì—ˆë‹¤ë©´ ë°˜í™˜
  if (globalImageAnalyzer?.getModelInfo().isInitialized) {
    return globalImageAnalyzer
  }

  // ì´ˆê¸°í™”ê°€ ì§„í–‰ ì¤‘ì´ë¼ë©´ ê¸°ë‹¤ë¦¼
  if (initializationPromise) {
    return await initializationPromise
  }

  // ìƒˆë¡œìš´ ì´ˆê¸°í™” ì‹œì‘
  initializationPromise = (async () => {
    try {
      // ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ê°€ ìˆë‹¤ë©´ ì •ë¦¬
      if (globalImageAnalyzer) {
        globalImageAnalyzer.dispose()
      }

      console.log('ğŸ”„ Creating new AI Image Analyzer instance...')
      globalImageAnalyzer = new AIImageAnalyzer()
      await globalImageAnalyzer.initialize()

      console.log('âœ… AI Image Analyzer ready for use')
      return globalImageAnalyzer
    } catch (error) {
      // ì´ˆê¸°í™” ì‹¤íŒ¨ì‹œ ì •ë¦¬
      globalImageAnalyzer = null
      initializationPromise = null
      throw error
    }
  })()

  try {
    const analyzer = await initializationPromise
    initializationPromise = null
    return analyzer
  } catch (error) {
    initializationPromise = null
    throw error
  }
}
